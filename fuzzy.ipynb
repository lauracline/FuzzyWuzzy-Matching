{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### libraries\n",
    " More information on fuzzywuzzy package https://github.com/seatgeek/fuzzywuzzy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import collections\n",
    "import time\n",
    "from fuzzywuzzy import fuzz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset\n",
    "Create dataset that consists name, address and unique ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {'Customer Name': ['ED SMITH','EDWARD SMITH','JOE DOE','PAT MUELLER','PATRICK MUELLER','LISA DANE', \n",
    "                       'ALEX SMITH', 'SMITH ALEX','DIANA HAYDEN','RUSS PARK','SARA JANE', 'AARON GOMSEY',\n",
    "                       'ARON GOMSEY','KIM CHEN','SCOTT HARRIS','ELIZABETH HURN','LIZ HURN','ROBERT LIN',\n",
    "                       'ANDY ROBERTSON','JORDAN HENDERSON'],\n",
    "     'Customer Address': ['218-KENSINGTON-DETROIT-MI','218-KENSINGTON-DETROIT-MI','SUNPHARMA-NOVI-MI',\n",
    "                          '7650-ASHWOODPARK-DALLAS-TX','7650-ASHWOODPARK-DALLAS-TX','2100-PEBBLEVALE-MIAMI-FL',\n",
    "                          '16-MISSIONSTREET-SANFRANCISCO-CA','16-MISSIONSTREET-SANFRANCISCO-CA','FARM-WOOD-AZ',\n",
    "                          'WEST-CAMBELL-LASVEGAS-NV','9-OAKS-APTS-BUFFALO-NY','11-DNAWAY-REDWOOD-CA',\n",
    "                          '11-DNAWAY-REDWOOD-CA','8-ELMOND-BOLINGBROOK-IL','9024-SOUTHSTREET-MONSEY-NY',\n",
    "                          '53-DOGWOOD-LIVINGSTON-NJ','53-DOGWOOD-LIVINGSTON-NJ','8505-CORONAAVE-BARBERTON-OH',\n",
    "                          '11-PULASKIAVE-EASTSTROUD-PA','161-SUMMITDR-WAUSAU-WI'],\n",
    "     'Customer ID': [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read nicknames csv file from below link\n",
    "#\"https://github.com/carltonnorthern/nickname-and-diminutive-names-lookup/blob/master/names.csv\"\n",
    "nick_df = pd.read_csv('Nicknames.csv', sep=',', header=0)\n",
    "print(nick_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#update duplicate dictionary, keys are ID that is kept in final data\n",
    "#vals are list of dups discarded as result of comparision with key\n",
    "def update_dups(dupdict,keep,discard):\n",
    "    if keep[2]!=discard[2] and discard[2] in dupdict:\n",
    "        for id in dupdict[discard[2]]:\n",
    "            dupdict[keep[2]].append(id)\n",
    "        del dupdict[discard[2]]\n",
    "    dupdict[keep[2]].append(discard[2])\n",
    "    return dupdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new function called dups that discards duplicates of higher ID number and creates a seperate dataframe \n",
    "#final resemble data without duplicates\n",
    "#discarded consist duplicate records data\n",
    "def dups(final, discard, people, key, dupdict):\n",
    "    p1=people[0]\n",
    "    if len(people)>1:\n",
    "        iterpeople=people[1:]\n",
    "        for p2 in iterpeople:\n",
    "            if (p1[0] in inverse and p2[0] in inverse[p1[0]]) or (p2[0] in inverse and p1[0] in inverse[p2[0]]):\n",
    "                if p1[2]<=p2[2]:\n",
    "                    dupdict=update_dups(dupdict,p1,p2)\n",
    "                    discard.append([p2[1],key,p2[2]])\n",
    "                    people.remove(p2)\n",
    "                else:\n",
    "                    dupdict=update_dups(dupdict,p2,p1)\n",
    "                    discard.append([p1[1],key,p1[2]])\n",
    "                    return dups(final,discard,people[1:],key,dupdict)\n",
    "        final.append([p1[1],key,p1[2]])\n",
    "        if len(people)>1:\n",
    "            return dups(final,discard,people[1:],key,dupdict)\n",
    "        else:\n",
    "            return final, discard, dupdict\n",
    "    else:\n",
    "        final.append([p1[1],key,p1[2]])\n",
    "        return final, discard, dupdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#updates duplicate dictionary, keys are ID that is kept in final data \n",
    "#vals are list of dups discarded as result of comparision with key\n",
    "def update_fuzzy(dupdict,keep,discard):\n",
    "    if keep[1]!=discard[1] and discard[1] in dupdict:\n",
    "        for id in dupdict[discard[1]]:\n",
    "            dupdict[keep[1]].append(id)\n",
    "        del dupdict[discard[1]]\n",
    "    dupdict[keep[1]].append(discard[1])\n",
    "    return dupdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define new function called dups that discards duplicates of higher ID number and creates a seperate dataframe \n",
    "#final resemble data without duplicates\n",
    "#discarded consist duplicate records data\n",
    "def fuzzydups(final, discard, people, key, dupdict):\n",
    "    p1=people[0]\n",
    "    if len(people)>1:\n",
    "        iterpeople=people[1:]\n",
    "        for p2 in iterpeople:\n",
    "            if fuzz.token_set_ratio(p1[0],p2[0]) > 80:\n",
    "                if p1[1]<=p2[1]:\n",
    "                    dupdict=update_fuzzy(dupdict,p1,p2)\n",
    "                    discard.append([p2[0],key,p2[1]])\n",
    "                    people.remove(p2)\n",
    "                else:\n",
    "                    dupdict=update_fuzzy(dupdict,p2,p1)\n",
    "                    discard.append([p1[0],key,p1[1]])\n",
    "                    return fuzzydups(final,discard,people[1:],key,dupdict)\n",
    "        final.append([p1[0],key,p1[1]])\n",
    "        if len(people)>1:\n",
    "            return fuzzydups(final,discard,people[1:],key,dupdict)\n",
    "        else:\n",
    "            return final, discard, dupdict\n",
    "    else:\n",
    "        final.append([p1[0],key,p1[1]])\n",
    "        return final, discard, dupdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create dict from nickname csv\n",
    "start_time = time.time() \n",
    "\n",
    "d={}\n",
    "for index, row in nick_df.iterrows():\n",
    "    s=row[0].split(',')\n",
    "    d[s[0]]=s[1:]\n",
    "    \n",
    "    \n",
    "#invert dictionary so that nicknames are keys\n",
    "inverse=dict()\n",
    "for key in d:\n",
    "    for val in d[key]:\n",
    "        if val not in inverse:\n",
    "            inverse[val]=[key]\n",
    "        else:\n",
    "            inverse[val].append(key)\n",
    "            \n",
    "\n",
    "#create dict from address to list of customers for nickname matching                    \n",
    "testdict={}\n",
    "for index,row in df.iterrows():\n",
    "    if row[1] not in testdict:\n",
    "        testdict[row[1]]=[[row[0].split(' ')[0].lower(),row[0],row[2]]]\n",
    "    else:\n",
    "        testdict[row[1]].append([row[0].split(' ')[0].lower(),row[0],row[2]])       \n",
    "\n",
    "\t\t\n",
    "#run nickname matching\n",
    "final=[]\n",
    "discarded=[]\n",
    "dupdict=collections.defaultdict(list)\n",
    "for key,val in testdict.items():\n",
    "    people=list(val)\n",
    "    f,d,dupdict=dups([],[],people,key,dupdict)\n",
    "    final+=f\n",
    "    discarded+=d\n",
    "newdf=pd.DataFrame(final, columns=['NAME', 'KEY', 'CUST_MSTR_ID'])\n",
    "dropped=pd.DataFrame(discarded, columns=['NAME', 'KEY', 'CUST_MSTR_ID'])\n",
    "\n",
    "\n",
    "\n",
    "fuzzydict={}\n",
    "for index,row in newdf.iterrows():\n",
    "    if row[1] not in fuzzydict:\n",
    "        fuzzydict[row[1]]=[[row[0],row[2]]]\n",
    "    else:\n",
    "        fuzzydict[row[1]].append([row[0],row[2]])\n",
    "\n",
    "\t\t\n",
    "#run fuzzy matching\n",
    "fuzzyfinal=[]\n",
    "fuzzydiscard=[]\n",
    "for key,val in fuzzydict.items():\n",
    "    people=list(val)\n",
    "    f,d,dupdict=fuzzydups([],[],people,key,dupdict)\n",
    "    fuzzyfinal+=f\n",
    "    fuzzydiscard+=d\n",
    "fuzzydf=pd.DataFrame(fuzzyfinal, columns=['NAME', 'KEY', 'CUST_MSTR_ID'])\n",
    "fuzzydropped=pd.DataFrame(fuzzydiscard, columns=['NAME', 'KEY', 'CUST_MSTR_ID'])\n",
    "\n",
    "print(\"My program took\", time.time() - start_time, \"to run\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combine records dropped in nickname/fuzzy matching\n",
    "alldropped=pd.concat([dropped,fuzzydropped], ignore_index=True)\n",
    "\n",
    "#add column to final data with all duplicate IDs for row     \n",
    "l=[]\n",
    "for index, row in fuzzydf.iterrows():\n",
    "    if row[2] in dupdict:\n",
    "        l.append(dupdict[row[2]])\n",
    "    else:\n",
    "        l.append([])\n",
    "fuzzydf['DUPLICATE_IDS']=pd.Series(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save file with final data after fuzzy matching\n",
    "writer = pd.ExcelWriter('fuzzy_final.xlsx', engine='xlsxwriter')\n",
    "fuzzydf.to_excel(writer, index=False)\n",
    "writer.save()\n",
    "\n",
    "#save file with all dropped IDs\n",
    "writer = pd.ExcelWriter('drop_final.xlsx', engine='xlsxwriter')\n",
    "alldropped.to_excel(writer, index=False)\n",
    "writer.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
